{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"../local_packages/nltk_data\")\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents = pd.read_pickle('./data/Patent_Dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>classifications_cpc</th>\n",
       "      <th>publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US06841689-20050111</td>\n",
       "      <td>There is provided a process for the addition o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2005-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US06895631-20050524</td>\n",
       "      <td>A buffing pad that includes a substantially co...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2005-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US06974769-20051213</td>\n",
       "      <td>Conductive structures in features of an insula...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2005-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US07006202-20060228</td>\n",
       "      <td>A mask holder for irradiating UV-rays is discl...</td>\n",
       "      <td>[G03B27/42, G03B27/58, G03B27/62]</td>\n",
       "      <td>2006-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US07096910-20060829</td>\n",
       "      <td>A tyre for a vehicle wheel includes a carcass ...</td>\n",
       "      <td>[B29D30/00, B60C9/00, B60C9/02]</td>\n",
       "      <td>2006-08-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             patent_id                                           abstract  \\\n",
       "0  US06841689-20050111  There is provided a process for the addition o...   \n",
       "1  US06895631-20050524  A buffing pad that includes a substantially co...   \n",
       "2  US06974769-20051213  Conductive structures in features of an insula...   \n",
       "3  US07006202-20060228  A mask holder for irradiating UV-rays is discl...   \n",
       "4  US07096910-20060829  A tyre for a vehicle wheel includes a carcass ...   \n",
       "\n",
       "                 classifications_cpc publication_date  \n",
       "0                                 []       2005-01-11  \n",
       "1                                 []       2005-05-24  \n",
       "2                                 []       2005-12-13  \n",
       "3  [G03B27/42, G03B27/58, G03B27/62]       2006-02-28  \n",
       "4    [B29D30/00, B60C9/00, B60C9/02]       2006-08-29  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our index is incorrect, quickly reset it\n",
    "patents = patents.reset_index(drop=True)\n",
    "\n",
    "patents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is provided a process for the addition of a nucleophile across an electron poor carbon-carbon double bond (a Michael addition) comprising contacting in a solvent: i) a nucleophile; ii) a compound comprising an electron poor double bond; and iii) a catalyst comprising a soluble polymer and a polyamino acid.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the text before we lowercase it\n",
    "# using the column \"abstract\" from the data frame and\n",
    "# observing the first row in that column (index 0)\n",
    "patents[\"abstract\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there is provided a process for the addition of a nucleophile across an electron poor carbon-carbon double bond (a michael addition) comprising contacting in a solvent: i) a nucleophile; ii) a compound comprising an electron poor double bond; and iii) a catalyst comprising a soluble polymer and a polyamino acid.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase the text using pandas string methods\n",
    "patents['abstract_lower'] = patents['abstract'].str.lower()\n",
    "\n",
    "# viewing the text after lowercasing\n",
    "patents[\"abstract_lower\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# display the standard string punctuation\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to create a regular expression (covered in the previous chapter)\n",
    "# which captures all the above punctuation characters\n",
    "\"[{}]\".format(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is provided a process for the addition of a nucleophile across an electron poor carbon-carbon double bond (a Michael addition) comprising contacting in a solvent: i) a nucleophile; ii) a compound comprising an electron poor double bond; and iii) a catalyst comprising a soluble polymer and a polyamino acid.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below is a function that uses regex to remove punctuation from strings\n",
    "def remove_punct(ptext):\n",
    "    # replace any punctuation with nothing \"\", effectively removing it\n",
    "    ptext = re.sub(string=ptext,\n",
    "                   pattern=\"[{}]\".format(string.punctuation), \n",
    "                   repl=\"\")\n",
    "    return ptext\n",
    "\n",
    "# by making a function that works for one piece of text\n",
    "# we can then apply the function to all the pandas text\n",
    "\n",
    "# viewing our text before removing punctuation\n",
    "patents[\"abstract\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is provided a process for the addition of a nucleophile across an electron poor carboncarbon double bond a Michael addition comprising contacting in a solvent i a nucleophile ii a compound comprising an electron poor double bond and iii a catalyst comprising a soluble polymer and a polyamino acid'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patents['abstract_no_punct'] = patents['abstract'].apply(remove_punct)\n",
    "\n",
    "patents['abstract_no_punct'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\t This is the text given\n",
      "Original text 'type':\n",
      "\t str\n",
      "Tokenized text:\n",
      "\t ['This', 'is', 'the', 'text', 'given']\n",
      "Tokenized text 'type':\n",
      "\t list\n"
     ]
    }
   ],
   "source": [
    "# Basic tokenization example\n",
    "original_text = \"This is the text given\"\n",
    "\n",
    "# Create tokens by splitting the text on each \" \" space\n",
    "tokens = original_text.split(\" \")\n",
    "\n",
    "print(\"Original text:\\n\\t\", original_text)\n",
    "print(\"Original text 'type':\\n\\t\", type(original_text).__name__)\n",
    "print(\"Tokenized text:\\n\\t\", tokens)\n",
    "print(\"Tokenized text 'type':\\n\\t\", type(tokens).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joyces\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'the', 'text', 'given']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the 'punkt' tokenizer data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Now you can use the word_tokenize function\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(original_text)\n",
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'is',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'process',\n",
       " 'for',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " 'across',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carbon-carbon',\n",
       " 'double',\n",
       " 'bond',\n",
       " '(',\n",
       " 'a',\n",
       " 'Michael',\n",
       " 'addition',\n",
       " ')',\n",
       " 'comprising',\n",
       " 'contacting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'solvent',\n",
       " ':',\n",
       " 'i',\n",
       " ')',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " ';',\n",
       " 'ii',\n",
       " ')',\n",
       " 'a',\n",
       " 'compound',\n",
       " 'comprising',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'double',\n",
       " 'bond',\n",
       " ';',\n",
       " 'and',\n",
       " 'iii',\n",
       " ')',\n",
       " 'a',\n",
       " 'catalyst',\n",
       " 'comprising',\n",
       " 'a',\n",
       " 'soluble',\n",
       " 'polymer',\n",
       " 'and',\n",
       " 'a',\n",
       " 'polyamino',\n",
       " 'acid',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply to 'abstract' column in dataframe\n",
    "patents['abstract_tokens'] = patents['abstract'].apply(nltk.word_tokenize)\n",
    "\n",
    "patents[\"abstract_tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is provided a process for the addition of a nucleophile across an electron poor carbon-carbon double bond (a Michael addition) comprising contacting in a solvent: i) a nucleophile; ii) a compound comprising an electron poor double bond; and iii) a catalyst comprising a soluble polymer and a polyamino acid.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply tokenisation to abstract column in dataframe\n",
    "patents['abstract_sentences'] = patents['abstract'].apply(nltk.sent_tokenize)\n",
    "\n",
    "# text before sentence segmentation\n",
    "patents[\"abstract\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is provided a process for the addition of a nucleophile across an electron poor carbon-carbon double bond (a Michael addition) comprising contacting in a solvent: i) a nucleophile; ii) a compound comprising an electron poor double bond; and iii) a catalyst comprising a soluble polymer and a polyamino acid.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text after sentence segmentation\n",
    "patents[\"abstract_sentences\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A buffing pad that includes a substantially conventional buffing pad made of tufted wool that includes a backing plate and a central hub for attachment to the shaft of a rotary power buffer.',\n",
       " 'The tufts or strands of wool are of a first and substantially equal length and of a substantially uniform color.',\n",
       " 'The pad also carries a plurality of shorter tufts of wool or other fiber of a contrasting color.',\n",
       " 'When the pad is relatively new, the shorter tufts are not normally visible as they are hidden by the longer tufts.',\n",
       " 'As the pad becomes worn, however, the longer tufts become shorter and matted down thereby exposing the shorter tufts.',\n",
       " 'The appearance of the contrasting color of the shorter tufts is an indicator that the pad is worn and should be replaced.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patents[\"abstract_sentences\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Ones',\n",
       " 'and',\n",
       " 'twos',\n",
       " 'argues',\n",
       " 'who',\n",
       " 'is',\n",
       " 'winning',\n",
       " 'in',\n",
       " 'todays',\n",
       " 'matches']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the PorterStemmer from nltk\n",
    "words_to_normalise = \"The Ones and twos argues who is winning in todays matches\"\n",
    "\n",
    "# generate tokens\n",
    "tokens = nltk.word_tokenize(words_to_normalise)\n",
    "\n",
    "# note we have not applied any other preprocessing to the text\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'one', 'and', 'two', 'argu', 'who', 'is', 'win', 'in', 'today', 'match']\n"
     ]
    }
   ],
   "source": [
    "# Loop through each token in the list and apply the stemming\n",
    "tokens_stemmed = [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "# A list of stemmed words\n",
    "print(tokens_stemmed)\n",
    "# The has clearly applied some processing to the text, beyond chopping\n",
    "# off an ending. This PorterStemmer has also lowercased the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'is',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'process',\n",
       " 'for',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " 'across',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carbon-carbon',\n",
       " 'double',\n",
       " 'bond',\n",
       " '(',\n",
       " 'a',\n",
       " 'Michael',\n",
       " 'addition',\n",
       " ')',\n",
       " 'comprising',\n",
       " 'contacting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'solvent',\n",
       " ':',\n",
       " 'i',\n",
       " ')',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " ';',\n",
       " 'ii',\n",
       " ')',\n",
       " 'a',\n",
       " 'compound',\n",
       " 'comprising',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'double',\n",
       " 'bond',\n",
       " ';',\n",
       " 'and',\n",
       " 'iii',\n",
       " ')',\n",
       " 'a',\n",
       " 'catalyst',\n",
       " 'comprising',\n",
       " 'a',\n",
       " 'soluble',\n",
       " 'polymer',\n",
       " 'and',\n",
       " 'a',\n",
       " 'polyamino',\n",
       " 'acid',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying stemming to the pandas data\n",
    "# Define stemming function\n",
    "\n",
    "def stemming(ptoken):\n",
    "    # create stemming object\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in ptoken]  \n",
    "\n",
    "# tokens pre-stemming\n",
    "patents['abstract_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there',\n",
       " 'is',\n",
       " 'provid',\n",
       " 'a',\n",
       " 'process',\n",
       " 'for',\n",
       " 'the',\n",
       " 'addit',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nucleophil',\n",
       " 'across',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carbon-carbon',\n",
       " 'doubl',\n",
       " 'bond',\n",
       " '(',\n",
       " 'a',\n",
       " 'michael',\n",
       " 'addit',\n",
       " ')',\n",
       " 'compris',\n",
       " 'contact',\n",
       " 'in',\n",
       " 'a',\n",
       " 'solvent',\n",
       " ':',\n",
       " 'i',\n",
       " ')',\n",
       " 'a',\n",
       " 'nucleophil',\n",
       " ';',\n",
       " 'ii',\n",
       " ')',\n",
       " 'a',\n",
       " 'compound',\n",
       " 'compris',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'doubl',\n",
       " 'bond',\n",
       " ';',\n",
       " 'and',\n",
       " 'iii',\n",
       " ')',\n",
       " 'a',\n",
       " 'catalyst',\n",
       " 'compris',\n",
       " 'a',\n",
       " 'solubl',\n",
       " 'polym',\n",
       " 'and',\n",
       " 'a',\n",
       " 'polyamino',\n",
       " 'acid',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patents['abstract_tokens_stemmed'] = patents['abstract_tokens'].apply(stemming)\n",
    "\n",
    "# tokens post stemming\n",
    "patents['abstract_tokens_stemmed'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'there'),\n",
       " ('is', 'is'),\n",
       " ('provided', 'provid'),\n",
       " ('a', 'a'),\n",
       " ('process', 'process'),\n",
       " ('for', 'for'),\n",
       " ('the', 'the'),\n",
       " ('addition', 'addit'),\n",
       " ('of', 'of'),\n",
       " ('a', 'a'),\n",
       " ('nucleophile', 'nucleophil'),\n",
       " ('across', 'across'),\n",
       " ('an', 'an'),\n",
       " ('electron', 'electron'),\n",
       " ('poor', 'poor'),\n",
       " ('carbon-carbon', 'carbon-carbon'),\n",
       " ('double', 'doubl'),\n",
       " ('bond', 'bond'),\n",
       " ('(', '('),\n",
       " ('a', 'a'),\n",
       " ('Michael', 'michael'),\n",
       " ('addition', 'addit'),\n",
       " (')', ')'),\n",
       " ('comprising', 'compris'),\n",
       " ('contacting', 'contact'),\n",
       " ('in', 'in'),\n",
       " ('a', 'a'),\n",
       " ('solvent', 'solvent'),\n",
       " (':', ':'),\n",
       " ('i', 'i'),\n",
       " (')', ')'),\n",
       " ('a', 'a'),\n",
       " ('nucleophile', 'nucleophil'),\n",
       " (';', ';'),\n",
       " ('ii', 'ii'),\n",
       " (')', ')'),\n",
       " ('a', 'a'),\n",
       " ('compound', 'compound'),\n",
       " ('comprising', 'compris'),\n",
       " ('an', 'an'),\n",
       " ('electron', 'electron'),\n",
       " ('poor', 'poor'),\n",
       " ('double', 'doubl'),\n",
       " ('bond', 'bond'),\n",
       " (';', ';'),\n",
       " ('and', 'and'),\n",
       " ('iii', 'iii'),\n",
       " (')', ')'),\n",
       " ('a', 'a'),\n",
       " ('catalyst', 'catalyst'),\n",
       " ('comprising', 'compris'),\n",
       " ('a', 'a'),\n",
       " ('soluble', 'solubl'),\n",
       " ('polymer', 'polym'),\n",
       " ('and', 'and'),\n",
       " ('a', 'a'),\n",
       " ('polyamino', 'polyamino'),\n",
       " ('acid', 'acid'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing the pre and post stemmed tokens\n",
    "list(zip(patents['abstract_tokens'][0], patents['abstract_tokens_stemmed'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Ones and twos argues who is winning in todays matches\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Ones',\n",
       " 'and',\n",
       " 'twos',\n",
       " 'argues',\n",
       " 'who',\n",
       " 'is',\n",
       " 'winning',\n",
       " 'in',\n",
       " 'todays',\n",
       " 'matches']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the WordNetLemmatizer from nltk\n",
    "# without using the parts of speech tagging it doesn't perform incredibly well\n",
    "# But it does always result in valid words where it runs\n",
    "\n",
    "# Run tokenizer\n",
    "tokens = nltk.word_tokenize(words_to_normalise)\n",
    "print(words_to_normalise)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\joyces\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ones', 'and', 'two', 'argues', 'who', 'is', 'winning', 'in', 'today', 'match']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the 'wordnet' corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Now you can use the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens_lemmed = [WordNetLemmatizer().lemmatize(token) for token in tokens]\n",
    "print(tokens_lemmed)\n",
    "type(tokens_lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'is',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'process',\n",
       " 'for',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " 'across',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carbon-carbon',\n",
       " 'double',\n",
       " 'bond',\n",
       " '(',\n",
       " 'a',\n",
       " 'Michael',\n",
       " 'addition',\n",
       " ')',\n",
       " 'comprising',\n",
       " 'contacting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'solvent',\n",
       " ':',\n",
       " 'i',\n",
       " ')',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " ';',\n",
       " 'ii',\n",
       " ')',\n",
       " 'a',\n",
       " 'compound',\n",
       " 'comprising',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'double',\n",
       " 'bond',\n",
       " ';',\n",
       " 'and',\n",
       " 'iii',\n",
       " ')',\n",
       " 'a',\n",
       " 'catalyst',\n",
       " 'comprising',\n",
       " 'a',\n",
       " 'soluble',\n",
       " 'polymer',\n",
       " 'and',\n",
       " 'a',\n",
       " 'polyamino',\n",
       " 'acid',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the lemmatize() function\n",
    "\n",
    "def lemmatise(ptokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in ptokens]\n",
    "\n",
    "# tokens before lemmatization\n",
    "patents[\"abstract_tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'is',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'process',\n",
       " 'for',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " 'across',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carbon-carbon',\n",
       " 'double',\n",
       " 'bond',\n",
       " '(',\n",
       " 'a',\n",
       " 'Michael',\n",
       " 'addition',\n",
       " ')',\n",
       " 'comprising',\n",
       " 'contacting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'solvent',\n",
       " ':',\n",
       " 'i',\n",
       " ')',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " ';',\n",
       " 'ii',\n",
       " ')',\n",
       " 'a',\n",
       " 'compound',\n",
       " 'comprising',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'double',\n",
       " 'bond',\n",
       " ';',\n",
       " 'and',\n",
       " 'iii',\n",
       " ')',\n",
       " 'a',\n",
       " 'catalyst',\n",
       " 'comprising',\n",
       " 'a',\n",
       " 'soluble',\n",
       " 'polymer',\n",
       " 'and',\n",
       " 'a',\n",
       " 'polyamino',\n",
       " 'acid',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply lemmatisation to all tokens in column\n",
    "patents['abstract_tokens_lemmatised'] = patents['abstract_tokens'].apply(lemmatise)\n",
    "\n",
    "# tokens after lemmatization\n",
    "patents[\"abstract_tokens_lemmatised\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'There'),\n",
       " ('is', 'is'),\n",
       " ('provided', 'provided'),\n",
       " ('a', 'a'),\n",
       " ('process', 'process'),\n",
       " ('for', 'for'),\n",
       " ('the', 'the'),\n",
       " ('addition', 'addition'),\n",
       " ('of', 'of'),\n",
       " ('a', 'a'),\n",
       " ('nucleophile', 'nucleophile'),\n",
       " ('across', 'across'),\n",
       " ('an', 'an'),\n",
       " ('electron', 'electron'),\n",
       " ('poor', 'poor'),\n",
       " ('carbon-carbon', 'carbon-carbon'),\n",
       " ('double', 'double'),\n",
       " ('bond', 'bond'),\n",
       " ('(', '('),\n",
       " ('a', 'a'),\n",
       " ('Michael', 'Michael'),\n",
       " ('addition', 'addition'),\n",
       " (')', ')'),\n",
       " ('comprising', 'comprising'),\n",
       " ('contacting', 'contacting'),\n",
       " ('in', 'in'),\n",
       " ('a', 'a'),\n",
       " ('solvent', 'solvent'),\n",
       " (':', ':'),\n",
       " ('i', 'i'),\n",
       " (')', ')'),\n",
       " ('a', 'a'),\n",
       " ('nucleophile', 'nucleophile'),\n",
       " (';', ';'),\n",
       " ('ii', 'ii'),\n",
       " (')', ')'),\n",
       " ('a', 'a'),\n",
       " ('compound', 'compound'),\n",
       " ('comprising', 'comprising'),\n",
       " ('an', 'an'),\n",
       " ('electron', 'electron'),\n",
       " ('poor', 'poor'),\n",
       " ('double', 'double'),\n",
       " ('bond', 'bond'),\n",
       " (';', ';'),\n",
       " ('and', 'and'),\n",
       " ('iii', 'iii'),\n",
       " (')', ')'),\n",
       " ('a', 'a'),\n",
       " ('catalyst', 'catalyst'),\n",
       " ('comprising', 'comprising'),\n",
       " ('a', 'a'),\n",
       " ('soluble', 'soluble'),\n",
       " ('polymer', 'polymer'),\n",
       " ('and', 'and'),\n",
       " ('a', 'a'),\n",
       " ('polyamino', 'polyamino'),\n",
       " ('acid', 'acid'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparison of normalisation\n",
    "list(zip(patents[\"abstract_tokens\"][0], patents[\"abstract_tokens_lemmatised\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joyces\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the 'stopwords' corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Display the basic stopwords given by nltk\n",
    "print(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'et', 'un', 'seraient', 'avais', 'sommes', 'ai', 'me', 'aurions', 'aurai', 'eusses', 'ces', 'serons', 'à', 'ayante', 'serez', 'eux', 'ton', 'étaient', 'moi', 'sa', 'pas', 'étée', 'eu', 'étions', 'es', 'avons', 'seriez', 'aies', 'étante', 'par', 'avec', 'qui', 'eue', 'auriez', 'étés', 'serions', 'ayants', 'auras', 't', 'eussions', 'nos', 'ayons', 'eurent', 'ayantes', 'pour', 'étais', 'fusses', 'ma', 'est', 'eût', 'on', 'mais', 'qu', 'se', 'as', 'tes', 'était', 'soyez', 'serais', 'ils', 'l', 'eut', 'suis', 'une', 'ta', 'ne', 'étants', 'seront', 'furent', 'aie', 'eussent', 'aurons', 'fussions', 'aurait', 'fusse', 'son', 'soyons', 'te', 'd', 'fûmes', 'fut', 'ses', 'tu', 'avait', 's', 'votre', 'eus', 'avions', 'étées', 'que', 'le', 'eusse', 'eussiez', 'serait', 'avaient', 'les', 'ce', 'sois', 'fussiez', 'nous', 'c', 'serai', 'il', 'avez', 'ayant', 'aurez', 'toi', 'j', 'ayez', 'soient', 'même', 'vous', 'je', 'mon', 'notre', 'm', 'fussent', 'sur', 'du', 'elle', 'eûtes', 'aviez', 'mes', 'seras', 'soit', 'lui', 'êtes', 'étantes', 'ou', 'sont', 'dans', 'aurais', 'aura', 'sera', 'y', 'vos', 'n', 'auraient', 'aux', 'de', 'au', 'des', 'leur', 'étiez', 'eûmes', 'la', 'été', 'ait', 'fûtes', 'fût', 'ont', 'en', 'aient', 'fus', 'eues', 'étant', 'auront'}\n"
     ]
    }
   ],
   "source": [
    "stop_words_french = set(stopwords.words('french'))\n",
    "print(stop_words_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del stop_words_french\n",
    "# let's ensure we are using English stopwords for this project\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove stopwords from list of tokens\n",
    "def clean_stopwords(tokens):\n",
    "    # define stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # loop through each token and if the word isn't in the set \n",
    "    # of stopwords keep it\n",
    "    return [item for item in tokens if item not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'is',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'process',\n",
       " 'for',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " 'across',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carbon-carbon',\n",
       " 'double',\n",
       " 'bond',\n",
       " '(',\n",
       " 'a',\n",
       " 'Michael',\n",
       " 'addition',\n",
       " ')',\n",
       " 'comprising',\n",
       " 'contacting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'solvent',\n",
       " ':',\n",
       " 'i',\n",
       " ')',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " ';',\n",
       " 'ii',\n",
       " ')',\n",
       " 'a',\n",
       " 'compound',\n",
       " 'comprising',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'double',\n",
       " 'bond',\n",
       " ';',\n",
       " 'and',\n",
       " 'iii',\n",
       " ')',\n",
       " 'a',\n",
       " 'catalyst',\n",
       " 'comprising',\n",
       " 'a',\n",
       " 'soluble',\n",
       " 'polymer',\n",
       " 'and',\n",
       " 'a',\n",
       " 'polyamino',\n",
       " 'acid',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre stopword removal\n",
    "patents['abstract_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'provided',\n",
       " 'process',\n",
       " 'addition',\n",
       " 'nucleophile',\n",
       " 'across',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carbon-carbon',\n",
       " 'double',\n",
       " 'bond',\n",
       " '(',\n",
       " 'Michael',\n",
       " 'addition',\n",
       " ')',\n",
       " 'comprising',\n",
       " 'contacting',\n",
       " 'solvent',\n",
       " ':',\n",
       " ')',\n",
       " 'nucleophile',\n",
       " ';',\n",
       " 'ii',\n",
       " ')',\n",
       " 'compound',\n",
       " 'comprising',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'double',\n",
       " 'bond',\n",
       " ';',\n",
       " 'iii',\n",
       " ')',\n",
       " 'catalyst',\n",
       " 'comprising',\n",
       " 'soluble',\n",
       " 'polymer',\n",
       " 'polyamino',\n",
       " 'acid',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patents['tokens_no_stops'] = patents['abstract_tokens'].apply(clean_stopwords)\n",
    "\n",
    "# Post stopword removal\n",
    "patents['tokens_no_stops'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'is', 'a', 'for', 'an', 'the', 'and', 'i', 'of']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These were the stopwords that were removed\n",
    "found_stopwords = []\n",
    "\n",
    "# go through each unique token\n",
    "for token in set(patents['abstract_tokens'][0]):\n",
    "    if token in stop_words:\n",
    "        found_stopwords.append(token)\n",
    "        \n",
    "found_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To provide a method for measuring a plane mirror or a curved surface mirror close to plane mirror for condensing hard X-rays or soft X-rays used in a radiation light facility, especially an elliptical or tubular object having a steep profile exceeding 1×10 −4  rad, ultra precisely with a precision on nano order or sub-nano order. Overall profile is measured by using overall profile data obtained from a Fizeau interferometer and stitching a plurality of micromeasurement data from a Michelson microinterferometer. A curved surface measured and a reference plane are measured simultaneously by the Fizeau interferometer, a plurality of pieces of partial profile data in a region narrower than the curved surface measured are acquired simultaneously by inclining the curved surface measured and the reference plane simultaneously and sequentially with respect to a reference plane, relative angle between the pieces of partial profile data is measured as the inclination angle of the reference plane, and adjoining pieces of partial profile data are stitched by utilizing coincidence between the inclination angle and an overlapped region thus obtaining overall profile data.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at a new text, row 21, which contains some digits\n",
    "patents[\"abstract\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'method',\n",
       " 'for',\n",
       " 'measuring',\n",
       " 'a',\n",
       " 'plane',\n",
       " 'mirror',\n",
       " 'or',\n",
       " 'a',\n",
       " 'curved',\n",
       " 'surface',\n",
       " 'mirror',\n",
       " 'close',\n",
       " 'to',\n",
       " 'plane',\n",
       " 'mirror',\n",
       " 'for',\n",
       " 'condensing',\n",
       " 'hard',\n",
       " 'X-rays',\n",
       " 'or',\n",
       " 'soft',\n",
       " 'X-rays',\n",
       " 'used',\n",
       " 'in',\n",
       " 'a',\n",
       " 'radiation',\n",
       " 'light',\n",
       " 'facility',\n",
       " ',',\n",
       " 'especially',\n",
       " 'an',\n",
       " 'elliptical',\n",
       " 'or',\n",
       " 'tubular',\n",
       " 'object',\n",
       " 'having',\n",
       " 'a',\n",
       " 'steep',\n",
       " 'profile',\n",
       " 'exceeding',\n",
       " '1×10',\n",
       " '−4',\n",
       " 'rad',\n",
       " ',',\n",
       " 'ultra',\n",
       " 'precisely',\n",
       " 'with',\n",
       " 'a',\n",
       " 'precision',\n",
       " 'on',\n",
       " 'nano',\n",
       " 'order',\n",
       " 'or',\n",
       " 'sub-nano',\n",
       " 'order',\n",
       " '.',\n",
       " 'Overall',\n",
       " 'profile',\n",
       " 'is',\n",
       " 'measured',\n",
       " 'by',\n",
       " 'using',\n",
       " 'overall',\n",
       " 'profile',\n",
       " 'data',\n",
       " 'obtained',\n",
       " 'from',\n",
       " 'a',\n",
       " 'Fizeau',\n",
       " 'interferometer',\n",
       " 'and',\n",
       " 'stitching',\n",
       " 'a',\n",
       " 'plurality',\n",
       " 'of',\n",
       " 'micromeasurement',\n",
       " 'data',\n",
       " 'from',\n",
       " 'a',\n",
       " 'Michelson',\n",
       " 'microinterferometer',\n",
       " '.',\n",
       " 'A',\n",
       " 'curved',\n",
       " 'surface',\n",
       " 'measured',\n",
       " 'and',\n",
       " 'a',\n",
       " 'reference',\n",
       " 'plane',\n",
       " 'are',\n",
       " 'measured',\n",
       " 'simultaneously',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Fizeau',\n",
       " 'interferometer',\n",
       " ',',\n",
       " 'a',\n",
       " 'plurality',\n",
       " 'of',\n",
       " 'pieces',\n",
       " 'of',\n",
       " 'partial',\n",
       " 'profile',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'region',\n",
       " 'narrower',\n",
       " 'than',\n",
       " 'the',\n",
       " 'curved',\n",
       " 'surface',\n",
       " 'measured',\n",
       " 'are',\n",
       " 'acquired',\n",
       " 'simultaneously',\n",
       " 'by',\n",
       " 'inclining',\n",
       " 'the',\n",
       " 'curved',\n",
       " 'surface',\n",
       " 'measured',\n",
       " 'and',\n",
       " 'the',\n",
       " 'reference',\n",
       " 'plane',\n",
       " 'simultaneously',\n",
       " 'and',\n",
       " 'sequentially',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'a',\n",
       " 'reference',\n",
       " 'plane',\n",
       " ',',\n",
       " 'relative',\n",
       " 'angle',\n",
       " 'between',\n",
       " 'the',\n",
       " 'pieces',\n",
       " 'of',\n",
       " 'partial',\n",
       " 'profile',\n",
       " 'data',\n",
       " 'is',\n",
       " 'measured',\n",
       " 'as',\n",
       " 'the',\n",
       " 'inclination',\n",
       " 'angle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reference',\n",
       " 'plane',\n",
       " ',',\n",
       " 'and',\n",
       " 'adjoining',\n",
       " 'pieces',\n",
       " 'of',\n",
       " 'partial',\n",
       " 'profile',\n",
       " 'data',\n",
       " 'are',\n",
       " 'stitched',\n",
       " 'by',\n",
       " 'utilizing',\n",
       " 'coincidence',\n",
       " 'between',\n",
       " 'the',\n",
       " 'inclination',\n",
       " 'angle',\n",
       " 'and',\n",
       " 'an',\n",
       " 'overlapped',\n",
       " 'region',\n",
       " 'thus',\n",
       " 'obtaining',\n",
       " 'overall',\n",
       " 'profile',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_num(ptokens):\n",
    "    return [token for token in ptokens if token.isalpha()]\n",
    "\n",
    "# Apply removal function\n",
    "patents['abstract_no_nums'] = patents['abstract_tokens'].apply(remove_num)\n",
    "\n",
    "# ensure you only use remove_num on tokenized text, otherwise it tokenizes every character.\n",
    "patents[\"abstract_tokens\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'method',\n",
       " 'for',\n",
       " 'measuring',\n",
       " 'a',\n",
       " 'plane',\n",
       " 'mirror',\n",
       " 'or',\n",
       " 'a',\n",
       " 'curved',\n",
       " 'surface',\n",
       " 'mirror',\n",
       " 'close',\n",
       " 'to',\n",
       " 'plane',\n",
       " 'mirror',\n",
       " 'for',\n",
       " 'condensing',\n",
       " 'hard',\n",
       " 'or',\n",
       " 'soft',\n",
       " 'used',\n",
       " 'in',\n",
       " 'a',\n",
       " 'radiation',\n",
       " 'light',\n",
       " 'facility',\n",
       " 'especially',\n",
       " 'an',\n",
       " 'elliptical',\n",
       " 'or',\n",
       " 'tubular',\n",
       " 'object',\n",
       " 'having',\n",
       " 'a',\n",
       " 'steep',\n",
       " 'profile',\n",
       " 'exceeding',\n",
       " 'rad',\n",
       " 'ultra',\n",
       " 'precisely',\n",
       " 'with',\n",
       " 'a',\n",
       " 'precision',\n",
       " 'on',\n",
       " 'nano',\n",
       " 'order',\n",
       " 'or',\n",
       " 'order',\n",
       " 'Overall',\n",
       " 'profile',\n",
       " 'is',\n",
       " 'measured',\n",
       " 'by',\n",
       " 'using',\n",
       " 'overall',\n",
       " 'profile',\n",
       " 'data',\n",
       " 'obtained',\n",
       " 'from',\n",
       " 'a',\n",
       " 'Fizeau',\n",
       " 'interferometer',\n",
       " 'and',\n",
       " 'stitching',\n",
       " 'a',\n",
       " 'plurality',\n",
       " 'of',\n",
       " 'micromeasurement',\n",
       " 'data',\n",
       " 'from',\n",
       " 'a',\n",
       " 'Michelson',\n",
       " 'microinterferometer',\n",
       " 'A',\n",
       " 'curved',\n",
       " 'surface',\n",
       " 'measured',\n",
       " 'and',\n",
       " 'a',\n",
       " 'reference',\n",
       " 'plane',\n",
       " 'are',\n",
       " 'measured',\n",
       " 'simultaneously',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Fizeau',\n",
       " 'interferometer',\n",
       " 'a',\n",
       " 'plurality',\n",
       " 'of',\n",
       " 'pieces',\n",
       " 'of',\n",
       " 'partial',\n",
       " 'profile',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'region',\n",
       " 'narrower',\n",
       " 'than',\n",
       " 'the',\n",
       " 'curved',\n",
       " 'surface',\n",
       " 'measured',\n",
       " 'are',\n",
       " 'acquired',\n",
       " 'simultaneously',\n",
       " 'by',\n",
       " 'inclining',\n",
       " 'the',\n",
       " 'curved',\n",
       " 'surface',\n",
       " 'measured',\n",
       " 'and',\n",
       " 'the',\n",
       " 'reference',\n",
       " 'plane',\n",
       " 'simultaneously',\n",
       " 'and',\n",
       " 'sequentially',\n",
       " 'with',\n",
       " 'respect',\n",
       " 'to',\n",
       " 'a',\n",
       " 'reference',\n",
       " 'plane',\n",
       " 'relative',\n",
       " 'angle',\n",
       " 'between',\n",
       " 'the',\n",
       " 'pieces',\n",
       " 'of',\n",
       " 'partial',\n",
       " 'profile',\n",
       " 'data',\n",
       " 'is',\n",
       " 'measured',\n",
       " 'as',\n",
       " 'the',\n",
       " 'inclination',\n",
       " 'angle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reference',\n",
       " 'plane',\n",
       " 'and',\n",
       " 'adjoining',\n",
       " 'pieces',\n",
       " 'of',\n",
       " 'partial',\n",
       " 'profile',\n",
       " 'data',\n",
       " 'are',\n",
       " 'stitched',\n",
       " 'by',\n",
       " 'utilizing',\n",
       " 'coincidence',\n",
       " 'between',\n",
       " 'the',\n",
       " 'inclination',\n",
       " 'angle',\n",
       " 'and',\n",
       " 'an',\n",
       " 'overlapped',\n",
       " 'region',\n",
       " 'thus',\n",
       " 'obtaining',\n",
       " 'overall',\n",
       " 'profile',\n",
       " 'data']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see there are no longer any tokens with digits in it\n",
    "# \"1x10\", \"-4\" have been removed\n",
    "patents[\"abstract_no_nums\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'is',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'process',\n",
       " 'for',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " 'across',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carbon-carbon',\n",
       " 'double',\n",
       " 'bond',\n",
       " '(',\n",
       " 'a',\n",
       " 'Michael',\n",
       " 'addition',\n",
       " ')',\n",
       " 'comprising',\n",
       " 'contacting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'solvent',\n",
       " ':',\n",
       " 'i',\n",
       " ')',\n",
       " 'a',\n",
       " 'nucleophile',\n",
       " ';',\n",
       " 'ii',\n",
       " ')',\n",
       " 'a',\n",
       " 'compound',\n",
       " 'comprising',\n",
       " 'an',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'double',\n",
       " 'bond',\n",
       " ';',\n",
       " 'and',\n",
       " 'iii',\n",
       " ')',\n",
       " 'a',\n",
       " 'catalyst',\n",
       " 'comprising',\n",
       " 'a',\n",
       " 'soluble',\n",
       " 'polymer',\n",
       " 'and',\n",
       " 'a',\n",
       " 'polyamino',\n",
       " 'acid',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_short_tokens(ptokens):\n",
    "    return [token for token in ptokens if len(token) > 2]\n",
    "\n",
    "# pre short word removal\n",
    "patents[\"abstract_tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There',\n",
       " 'provided',\n",
       " 'process',\n",
       " 'for',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'nucleophile',\n",
       " 'across',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carbon-carbon',\n",
       " 'double',\n",
       " 'bond',\n",
       " 'Michael',\n",
       " 'addition',\n",
       " 'comprising',\n",
       " 'contacting',\n",
       " 'solvent',\n",
       " 'nucleophile',\n",
       " 'compound',\n",
       " 'comprising',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'double',\n",
       " 'bond',\n",
       " 'and',\n",
       " 'iii',\n",
       " 'catalyst',\n",
       " 'comprising',\n",
       " 'soluble',\n",
       " 'polymer',\n",
       " 'and',\n",
       " 'polyamino',\n",
       " 'acid']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the short word removal\n",
    "patents['abstract_no_small'] = patents['abstract_tokens'].apply(remove_short_tokens)\n",
    "\n",
    "# after removal\n",
    "patents[\"abstract_no_small\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_with_lemmatisation(raw_data):\n",
    "    \"\"\"Function to perform all preprocessing steps with lemmatisation\"\"\"\n",
    "    ptext = raw_data.lower()\n",
    "    ptext = remove_punct(ptext)\n",
    "    ptext = nltk.word_tokenize(ptext)\n",
    "    ptext = lemmatise(ptext)\n",
    "    ptext = remove_num(ptext)\n",
    "    ptext = clean_stopwords(ptext)\n",
    "    ptext = remove_short_tokens(ptext)\n",
    "\n",
    "    return ptext\n",
    "\n",
    "def preprocessing_with_stemming(raw_data):\n",
    "    \"\"\"Function to perform all preprocessing steps with stemming\"\"\"\n",
    "    ptext = raw_data.lower()\n",
    "    ptext = remove_punct(ptext)\n",
    "    ptext = nltk.word_tokenize(ptext)\n",
    "    ptext = stemming(ptext)\n",
    "    ptext = remove_num(ptext)\n",
    "    ptext = clean_stopwords(ptext)\n",
    "    ptext = remove_short_tokens(ptext)\n",
    "        \n",
    "    return ptext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provided',\n",
       " 'process',\n",
       " 'addition',\n",
       " 'nucleophile',\n",
       " 'across',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carboncarbon',\n",
       " 'double',\n",
       " 'bond',\n",
       " 'michael',\n",
       " 'addition',\n",
       " 'comprising',\n",
       " 'contacting',\n",
       " 'solvent',\n",
       " 'nucleophile',\n",
       " 'compound',\n",
       " 'comprising',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'double',\n",
       " 'bond',\n",
       " 'iii',\n",
       " 'catalyst',\n",
       " 'comprising',\n",
       " 'soluble',\n",
       " 'polymer',\n",
       " 'polyamino',\n",
       " 'acid']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform all processing at once\n",
    "patents['processed_with_lem'] = patents['abstract'].apply(preprocessing_with_lemmatisation)\n",
    "\n",
    "patents[\"processed_with_lem\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['provid',\n",
       " 'process',\n",
       " 'addit',\n",
       " 'nucleophil',\n",
       " 'across',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'carboncarbon',\n",
       " 'doubl',\n",
       " 'bond',\n",
       " 'michael',\n",
       " 'addit',\n",
       " 'compris',\n",
       " 'contact',\n",
       " 'solvent',\n",
       " 'nucleophil',\n",
       " 'compound',\n",
       " 'compris',\n",
       " 'electron',\n",
       " 'poor',\n",
       " 'doubl',\n",
       " 'bond',\n",
       " 'iii',\n",
       " 'catalyst',\n",
       " 'compris',\n",
       " 'solubl',\n",
       " 'polym',\n",
       " 'polyamino',\n",
       " 'acid']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patents['processed_with_stem'] = patents['abstract'].apply(preprocessing_with_stemming)\n",
    "\n",
    "patents[\"processed_with_stem\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('grad_prog')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f6f3da5bc72575a39b7e81cb6312df076a4203b1aa85d70ecab8d73eee0c231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
